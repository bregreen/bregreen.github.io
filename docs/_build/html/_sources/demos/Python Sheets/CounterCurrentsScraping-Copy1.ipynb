{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Counter-Currents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from   collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "import string \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from glob import glob\n",
    "import statsmodels.stats.api as sms\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Call the Main page of interest - Counter Currents\n",
    "mainpageURL = 'https://counter-currents.com'\n",
    "page = requests.get(mainpageURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check Page Pull Success\n",
    "def PageSuccess(page):\n",
    "    if page.status_code == 200:\n",
    "        print('Success!')\n",
    "    else:\n",
    "        print(\"Page error occured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "PageSuccess(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Beautiful Soup on Main page\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "## Pulling URLs for archives section into list\n",
    "archives = soup.find(id=\"archives-2\")\n",
    "#print(archives.prettify())\n",
    "archive_months = archives.find_all('li')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many months worth of archives did I pull?:  126 \n",
      " Years:  10.5\n"
     ]
    }
   ],
   "source": [
    "## How many months of archives did I pull?\n",
    "\n",
    "print(\"How many months worth of archives did I pull?: \", len(archive_months), '\\n', \"Years: \", len(archive_months)/12)\n",
    "#display(archive_months)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each month in archive, pull link into list and provide details if wanted/needed\n",
    "\n",
    "archive_url_list = []\n",
    "\n",
    "for a_month in archive_months:\n",
    "    link = a_month.find('a')['href']\n",
    "    archive_url_list.append(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does this count match the above month count?:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Does this count match the above month count?: \", len(archive_url_list)==len(archive_months))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each article in the archive month list, get url\n",
    "\n",
    "\n",
    "article_url_list_ = []\n",
    "\n",
    "for URL in archive_url_list:\n",
    "    soup2 = BeautifulSoup(requests.get(URL).content, 'html.parser')\n",
    "    articles = soup2.find_all('h2', class_=\"entry-title\")\n",
    "    for art in articles:\n",
    "        link_a = art.find('a')['href']\n",
    "        article_url_list_.append(link_a)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many article urls did I pull?:  3780 \n",
      "\n",
      " ['https://counter-currents.com/2020/11/the-counter-currents-2020-fundraiser-our-10000-matching-grant-livestreams-with-millennial-woes-endeavour/', 'https://counter-currents.com/2020/11/sun-and-steel/', 'https://counter-currents.com/2020/11/fox-news-the-boomer-question/']\n"
     ]
    }
   ],
   "source": [
    "## How many article urls did I pull?\n",
    "\n",
    "print(\"How many article urls did I pull?: \", len(article_url_list_), '\\n'*2, article_url_list_[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2020-11-29\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "today = str(date.today())\n",
    "print(\"Today's date:\", today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save URL list so I don't repeat\n",
    "a_urllist = pd.DataFrame(article_url_list_)\n",
    "a_urllist.to_csv(r'urllist' + today + '.csv', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = {}\n",
    "\n",
    "for URL in article_url_list_[:5]:\n",
    "    url = URL\n",
    "    soup3 = BeautifulSoup(requests.get(URL).content, 'html.parser')\n",
    "    title = soup3.find('h1', class_=\"entry-title\").text\n",
    "    text = soup3.find('div', class_=\"entry-content\").text\n",
    "    articledeets = soup3.find('div', class_=\"entry-utility\")\n",
    "    pubdate = articledeets.find('span', class_='entry-date date updated').text\n",
    "    origpubtag = articledeets.find('span', class_='cat-links').text\n",
    "    taglinks = articledeets.find('span', class_='tag-links')\n",
    "    tags = taglinks.find_all('a')\n",
    "    tagslist = []\n",
    "    for link in tags:\n",
    "        addtag = link.get_text()\n",
    "        tagslist.append(addtag)\n",
    "    text_data[title] = {}\n",
    "    text_data[title]['url'] = url\n",
    "    text_data[title]['content'] = text\n",
    "    text_data[title]['pubdate'] = pubdate\n",
    "    text_data[title]['pubbed_in'] = origpubtag\n",
    "    text_data[title]['other_tags'] = tagslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for URL in article_url_list_[5:10]:\n",
    "    url = URL\n",
    "    soup3 = BeautifulSoup(requests.get(URL).content, 'html.parser')\n",
    "    title = soup3.find('h1', class_=\"entry-title\").text\n",
    "    text = soup3.find('div', class_=\"entry-content\").text\n",
    "    articledeets = soup3.find('div', class_=\"entry-utility\")\n",
    "    pubdate = articledeets.find('span', class_='entry-date date updated').text\n",
    "    origpubtag = articledeets.find('span', class_='cat-links').text\n",
    "    taglinks = articledeets.find('span', class_='tag-links')\n",
    "    tags = taglinks.find_all('a')\n",
    "    tagslist = []\n",
    "    for link in tags:\n",
    "        addtag = link.get_text()\n",
    "        tagslist.append(addtag)\n",
    "    text_data[title] = {}\n",
    "    text_data[title]['url'] = url\n",
    "    text_data[title]['content'] = text\n",
    "    text_data[title]['pubdate'] = pubdate\n",
    "    text_data[title]['pubbed_in'] = origpubtag\n",
    "    text_data[title]['other_tags'] = tagslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text = pd.DataFrame(text_data).T\n",
    "#a_text['wc_cc'], a_text['editedtext'] = a_text['wc_text_listtuple'].str[0], a_text['wc_text_listtuple'].str[1]\n",
    "\n",
    "#pulled at date save to txt & csv\n",
    "a_text.to_csv(r'CounterCurrentsData500_2_pulled_' + today + '.txt')\n",
    "a_text.to_csv(r'CounterCurrentsData500_2_pulled_' + today + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text from each article\n",
    "\n",
    "text_data = {}\n",
    "\n",
    "for URL in article_url_list_[:500]:\n",
    "    url = URL\n",
    "    #sepcomments = 'If you want to support Counter-Currents'\n",
    "    #sepwc = 'words'\n",
    "    soup3 = BeautifulSoup(requests.get(URL).content, 'html.parser')\n",
    "    title = soup3.find('h1', class_=\"entry-title\").text\n",
    "    text = soup3.find('div', class_=\"entry-content\").text\n",
    "    title2 = soup3.title.text\n",
    "    #t2 = text.split(sepcomments, 1)[0]\n",
    "    #t3 = t2.split(sepwc, 1)\n",
    "    text_data[title] = {}\n",
    "    text_data[title]['title2'] = title2\n",
    "    text_data[title]['url'] = url\n",
    "    text_data[title]['content'] = text\n",
    "    #text_data[title]['text_unedited'] = t2\n",
    "    #text_data[title]['wc_text_listtuple'] = t3\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data2 = {}\n",
    "for URL in article_url_list_[500:1000]:\n",
    "    url = URL\n",
    "    #sepcomments = 'If you want to support Counter-Currents'\n",
    "    #sepwc = 'words'\n",
    "    soup3 = BeautifulSoup(requests.get(URL).content, 'html.parser')\n",
    "    title = soup3.find('h1', class_=\"entry-title\").text\n",
    "    text = soup3.find('div', class_=\"entry-content\").text\n",
    "    title2 = soup3.title.text\n",
    "    #t2 = text.split(sepcomments, 1)[0]\n",
    "    #t3 = t2.split(sepwc, 1)\n",
    "    #if (text.find('If you want to support Counter-Currents') != -1): \n",
    "    #    nocomments = text.split(sepcomments, 1)[0]\n",
    "    #else: \n",
    "    #    nocomments = text\n",
    "    text_data[title] = {}\n",
    "    text_data[title]['title2'] = title2\n",
    "    text_data[title]['url'] = url\n",
    "    text_data[title]['content'] = text\n",
    "    #text_data[title]['text_unedited'] = t2\n",
    "    #text_data[title]['wc_text_listtuple'] = t3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text2 = pd.DataFrame(text_data2).T\n",
    "#a_text2['wc_cc'], a_text2['editedtext'] = a_text2['wc_text_listtuple'].str[0], a_text2['wc_text_listtuple'].str[1]\n",
    "\n",
    "#pulled at date save to txt & csv\n",
    "a_text2.to_csv(r'CounterCurrentsData500_1000_pulled_' + today + '.txt')\n",
    "a_text2.to_csv(r'CounterCurrentsData500_1000_pulled_' + today + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data3 = {}\n",
    "for URL in article_url_list_[1000:1500]:\n",
    "    url = URL\n",
    "    #sepcomments = 'If you want to support Counter-Currents'\n",
    "    #sepwc = 'words'\n",
    "    soup3 = BeautifulSoup(requests.get(URL).content, 'html.parser')\n",
    "    title = soup3.find('h1', class_=\"entry-title\").text\n",
    "    text = soup3.find('div', class_=\"entry-content\").text\n",
    "    title2 = soup3.title.text\n",
    "    #t2 = text.split(sepcomments, 1)[0]\n",
    "    #t3 = t2.split(sepwc, 1)\n",
    "    #if (text.find('If you want to support Counter-Currents') != -1): \n",
    "    #    nocomments = text.split(sepcomments, 1)[0]\n",
    "    #else: \n",
    "    #    nocomments = text\n",
    "    text_data[title] = {}\n",
    "    text_data[title]['title2'] = title2\n",
    "    text_data[title]['url'] = url\n",
    "    text_data[title]['content'] = text\n",
    "    #text_data[title]['text_unedited'] = t2\n",
    "    #text_data[title]['wc_text_listtuple'] = t3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text3 = pd.DataFrame(text_data3).T\n",
    "#a_text3['wc_cc'], a_text3['editedtext'] = a_text3['wc_text_listtuple'].str[0], a_text3['wc_text_listtuple'].str[1]\n",
    "\n",
    "#pulled at date save to txt & csv\n",
    "a_text3.to_csv(r'CounterCurrentsData1000_1500_pulled_' + today + '.txt')\n",
    "a_text3.to_csv(r'CounterCurrentsData1000_1500_pulled_' + today + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data4 = {}\n",
    "for URL in article_url_list_[1500:2000]:\n",
    "    url = URL\n",
    "    #sepcomments = 'If you want to support Counter-Currents'\n",
    "    #sepwc = 'words'\n",
    "    soup3 = BeautifulSoup(requests.get(URL).content, 'html.parser')\n",
    "    title = soup3.find('h1', class_=\"entry-title\").text\n",
    "    text = soup3.find('div', class_=\"entry-content\").text\n",
    "    title2 = soup3.title.text\n",
    "    #t2 = text.split(sepcomments, 1)[0]\n",
    "    #t3 = t2.split(sepwc, 1)\n",
    "    #if (text.find('If you want to support Counter-Currents') != -1): \n",
    "    #    nocomments = text.split(sepcomments, 1)[0]\n",
    "    #else: \n",
    "    #    nocomments = text\n",
    "    text_data[title] = {}\n",
    "    text_data[title]['title2'] = title2\n",
    "    text_data[title]['url'] = url\n",
    "    text_data[title]['content'] = text\n",
    "    #text_data[title]['text_unedited'] = t2\n",
    "    #text_data[title]['wc_text_listtuple'] = t3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text4 = pd.DataFrame(text_data4).T\n",
    "#a_text4['wc_cc'], a_text4['editedtext'] = a_text4['wc_text_listtuple'].str[0], a_text4['wc_text_listtuple'].str[1]\n",
    "\n",
    "#pulled at date save to txt & csv\n",
    "a_text4.to_csv(r'CounterCurrentsData1500_2000_pulled_' + today + '.txt')\n",
    "a_text4.to_csv(r'CounterCurrentsData1500_2000_pulled_' + today + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data5 = {}\n",
    "for URL in article_url_list_[2000:2500]:\n",
    "    url = URL\n",
    "    #sepcomments = 'If you want to support Counter-Currents'\n",
    "    #sepwc = 'words'\n",
    "    soup3 = BeautifulSoup(requests.get(URL).content, 'html.parser')\n",
    "    title = soup3.find('h1', class_=\"entry-title\").text\n",
    "    text = soup3.find('div', class_=\"entry-content\").text\n",
    "    title2 = soup3.title.text\n",
    "    #t2 = text.split(sepcomments, 1)[0]\n",
    "    #t3 = t2.split(sepwc, 1)\n",
    "    #if (text.find('If you want to support Counter-Currents') != -1): \n",
    "    #    nocomments = text.split(sepcomments, 1)[0]\n",
    "    #else: \n",
    "    #    nocomments = text\n",
    "    text_data[title] = {}\n",
    "    text_data[title]['title2'] = title2\n",
    "    text_data[title]['url'] = url\n",
    "    text_data[title]['content'] = text\n",
    "    #text_data[title]['text_unedited'] = t2\n",
    "    #text_data[title]['wc_text_listtuple'] = t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text5 = pd.DataFrame(text_data5).T\n",
    "#a_text5['wc_cc'], a_text5['editedtext'] = a_text5['wc_text_listtuple'].str[0], a_text5['wc_text_listtuple'].str[1]\n",
    "\n",
    "#pulled at date save to txt & csv\n",
    "a_text5.to_csv(r'CounterCurrentsData2000_2500_pulled_' + today + '.txt')\n",
    "a_text5.to_csv(r'CounterCurrentsData2000_2500_pulled_' + today + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data6 = {}\n",
    "for URL in article_url_list_[2500:3000]:\n",
    "    url = URL\n",
    "    #sepcomments = 'If you want to support Counter-Currents'\n",
    "    #sepwc = 'words'\n",
    "    soup3 = BeautifulSoup(requests.get(URL).content, 'html.parser')\n",
    "    title = soup3.find('h1', class_=\"entry-title\").text\n",
    "    text = soup3.find('div', class_=\"entry-content\").text\n",
    "    title2 = soup3.title.text\n",
    "    #t2 = text.split(sepcomments, 1)[0]\n",
    "    #t3 = t2.split(sepwc, 1)\n",
    "    #if (text.find('If you want to support Counter-Currents') != -1): \n",
    "    #    nocomments = text.split(sepcomments, 1)[0]\n",
    "    #else: \n",
    "    #    nocomments = text\n",
    "    text_data[title] = {}\n",
    "    text_data[title]['title2'] = title2\n",
    "    text_data[title]['url'] = url\n",
    "    text_data[title]['content'] = text\n",
    "    #text_data[title]['text_unedited'] = t2\n",
    "    #text_data[title]['wc_text_listtuple'] = t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text6 = pd.DataFrame(text_data6).T\n",
    "#a_text6['wc_cc'], a_text6['editedtext'] = a_text6['wc_text_listtuple'].str[0], a_text6['wc_text_listtuple'].str[1]\n",
    "\n",
    "#pulled at date save to txt & csv\n",
    "a_text6.to_csv(r'CounterCurrentsData2500_3000_pulled_' + today + '.txt')\n",
    "a_text6.to_csv(r'CounterCurrentsData2500_3000_pulled_' + today + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data7 = {}\n",
    "for URL in article_url_list_[3000:3500]:\n",
    "    url = URL\n",
    "    #sepcomments = 'If you want to support Counter-Currents'\n",
    "    #sepwc = 'words'\n",
    "    soup3 = BeautifulSoup(requests.get(URL).content, 'html.parser')\n",
    "    title = soup3.find('h1', class_=\"entry-title\").text\n",
    "    text = soup3.find('div', class_=\"entry-content\").text\n",
    "    title2 = soup3.title.text\n",
    "    #t2 = text.split(sepcomments, 1)[0]\n",
    "    #t3 = t2.split(sepwc, 1)\n",
    "    #if (text.find('If you want to support Counter-Currents') != -1): \n",
    "    #    nocomments = text.split(sepcomments, 1)[0]\n",
    "    #else: \n",
    "    #    nocomments = text\n",
    "    text_data[title] = {}\n",
    "    text_data[title]['title2'] = title2\n",
    "    text_data[title]['url'] = url\n",
    "    text_data[title]['content'] = text\n",
    "    #text_data[title]['text_unedited'] = t2\n",
    "    #text_data[title]['wc_text_listtuple'] = t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text7 = pd.DataFrame(text_data7).T\n",
    "#a_text7['wc_cc'], a_text7['editedtext'] = a_text7['wc_text_listtuple'].str[0], a_text7['wc_text_listtuple'].str[1]\n",
    "\n",
    "#pulled at date save to txt & csv\n",
    "a_text7.to_csv(r'CounterCurrentsData3000_3500_pulled_' + today + '.txt')\n",
    "a_text7.to_csv(r'CounterCurrentsData3000_3500_pulled_' + today + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data8 = {}\n",
    "for URL in article_url_list_[3500:]:\n",
    "    url = URL\n",
    "    #sepcomments = 'If you want to support Counter-Currents'\n",
    "    #sepwc = 'words'\n",
    "    soup3 = BeautifulSoup(requests.get(URL).content, 'html.parser')\n",
    "    title = soup3.find('h1', class_=\"entry-title\").text\n",
    "    text = soup3.find('div', class_=\"entry-content\").text\n",
    "    title2 = soup3.title.text\n",
    "    #t2 = text.split(sepcomments, 1)[0]\n",
    "    #t3 = t2.split(sepwc, 1)\n",
    "    #if (text.find('If you want to support Counter-Currents') != -1): \n",
    "    #    nocomments = text.split(sepcomments, 1)[0]\n",
    "    #else: \n",
    "    #    nocomments = text\n",
    "    text_data[title] = {}\n",
    "    text_data[title]['title2'] = title2\n",
    "    text_data[title]['url'] = url\n",
    "    text_data[title]['content'] = text\n",
    "    #text_data[title]['text_unedited'] = t2\n",
    "    #text_data[title]['wc_text_listtuple'] = t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text8 = pd.DataFrame(text_data8).T\n",
    "#a_text8['wc_cc'], a_text8['editedtext'] = a_text8['wc_text_listtuple'].str[0], a_text8['wc_text_listtuple'].str[1]\n",
    "\n",
    "#pulled at date save to txt & csv\n",
    "a_text8.to_csv(r'CounterCurrentsData3500plus_pulled_' + today + '.txt')\n",
    "a_text8.to_csv(r'CounterCurrentsData3500plus_pulled_' + today + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = pd.concat([a_text,a_text2,a_text3,a_text4,a_text5,a_text6,a_text7,a_text8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(concat['url'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "#val = concat\n",
    "#display(val)\n",
    "display(concat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat.to_pickle(r'CC_all_pickle' + today + '.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
